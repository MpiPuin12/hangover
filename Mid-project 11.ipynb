{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e64c29",
   "metadata": {},
   "source": [
    "# MODEL 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028e1aa",
   "metadata": {},
   "source": [
    "- Setting Zipcode as dummie \n",
    "- Adding a new year column (either the construction date or the reform date if reformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed87db",
   "metadata": {},
   "source": [
    "## Model 1 using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408e9ae",
   "metadata": {},
   "source": [
    "# Data Cleaning & Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447ed05",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('pca', \n",
    "                  PCA(n_components=4)),('slr', LinearRegression())])\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('regression_data_decade.xls')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953503fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84965c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a8f1c",
   "metadata": {},
   "source": [
    "## Checking Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f444cd9",
   "metadata": {},
   "source": [
    "Our first step was to try to find the null values. The dataset doesn't have any null values  so we don't have to deal with them. However, it is important to think about the reason why these nulls do not exist, since this can introduce some kind of bias in the data. We have to observe if data may have been duplicated to avoid nulls, if random values have been incorporated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b396666",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ebbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.notnull().sum() # We haven't detected null values in any columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793d9b1",
   "metadata": {},
   "source": [
    "## Checking for duplicated Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec885ce7",
   "metadata": {},
   "source": [
    "Our approach to finding null values was first of all checking the reason why a same id was repeated. Due to the fact that we only had data from 2014 and 2015, probably the only reason why a house may be repeated it's because it was sold two times in this period and, therefore, two different prices (but the independent variables remained the same.) That's why we decided to only keep the last date transaction info since it's the one that recaps better the actual price of that home. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f586423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.duplicated().sum() #checking how many duplicated ids(Houses) are there in the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c693488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() #Checking if there are duplicated rows. There are not any identical rows so the duplicated Ids may have some difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.id.duplicated(),:].sort_values(by=['id']) # We check all the duplicated Ids in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecce793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "df[df.duplicated(['id'], keep=False)].sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478beac",
   "metadata": {},
   "source": [
    "We check for all the the id that are duplicated in all the columns except the price and drop the oldest date of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af72350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dupl=df[df.duplicated(['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'], keep=False)].sort_values(by=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ac0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates( subset=['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c94966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a254d",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d6933",
   "metadata": {},
   "source": [
    "## Checking data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef0189",
   "metadata": {},
   "source": [
    "We first thought of flooring the bathrooms to an integer but since it can also take decimals values we decided to leave it as a float. However, we did convert floors into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['floors']=df['floors'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69130ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21ecee",
   "metadata": {},
   "source": [
    "## Checking data shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883663ba",
   "metadata": {},
   "source": [
    "We first plot all the graphs to try to detect clear outliers. At first sight, most of the numerical columns( sqft_living, sqft_living15, sqft_lot, sqft_lot15, sqft_above, sqft_basement) seem to have some outliers but we'll get deeper into it by plotting also the scatterplot. For the categorical variables such as bedrooms, we'll deal with non-sense outliers such as 33 and 11 bedrooms(not consistent with the rest of the attributes of the house). For the 33 bedrooms house, we'll treat it as a typo and interpret it as 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,15),bins=20,layout=(6,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594330ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['bedrooms'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f984f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms'] = df['bedrooms'].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['bedrooms']==11].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['bedrooms'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495b934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hist(['floors'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8862cd0",
   "metadata": {},
   "source": [
    "## Check useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43929c7",
   "metadata": {},
   "source": [
    "For checking which columns should we add to our model, we run both the correlation matrix and the scatter_matrix so that we could check for multicollinearity, which were the variables more related to the price...\n",
    "After observing the scatter_matrix, and observing that the sqft_living behaved as a kind of normal distribution we decided to deal with it's outliers by droping the values away from it's mean and 3 std. deviations. For the numeric variables, we introduce to the model sqft_living and sqft_basement. The reason for the first (sqft_living) is that is the variable more correlated with the target and it's very correlated with another numeric variable (sqft_above) that we drop to avoid multicollinearity. Regarding sqft_basement, is not as correlated to sqft_living so we'll live it in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915176fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=df.drop(['date','lat','long','yr_built','zipcode','yr_renovated','grade','condition','view','waterfront'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corre_matrix=corr_df.corr()\n",
    "corre_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "mask=np.zeros_like(corre_matrix)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "fig,ax = plt.subplots(figsize=(14,10))\n",
    "ax=sns.heatmap(corre_matrix, mask=mask, center=0, cmap=sns.diverging_palette(220, 20, as_cmap=True), annot=True,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "df=df[(np.abs(stats.zscore(df['sqft_living'])) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.loc[df['sqft_lot']>1000000].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30627fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num=df[['sqft_living','bedrooms','bathrooms','floors', 'sqft_basement']]\n",
    "X_num_scatter_matrix=df[['sqft_above','sqft_living','sqft_basement','sqft_lot' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(X_num_scatter_matrix,alpha=0.2, figsize=(20,12), diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518de839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail().sort_values('sqft_lot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ee251",
   "metadata": {},
   "source": [
    "## Dealing with the categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fac34",
   "metadata": {},
   "source": [
    "As we commented before, we added a categorical variable(Decade build) that shows wither the date it was build or the date it was renovated. We afterwards group them by decades. We afterwards dummified all our categorical variables. This was one of the main limitations of this model since when dummifying the zipcode we creat a lot of little subsamples, some of them with not many observations which can drive us to error. This problem gets bigger when we drop some outliers that may reduce even more the size of our subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af372983",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_crosstab=pd.crosstab(df['condition'],df['grade'], margins=False)\n",
    "feat_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d365aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency \n",
    "chi2_contingency(feat_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_crosstab=pd.crosstab(df['view'],df['grade'], margins=False)\n",
    "feat_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ab342",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(feat_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat1=df[['grade', 'Decade Build', 'condition','grade','view']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies1=pd.get_dummies(X_cat1, drop_first=True)\n",
    "X_dummies1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e043d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat2=df[['zipcode']].astype(str)\n",
    "X_cat2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies2=pd.get_dummies(X_cat2, drop_first=True)\n",
    "X_dummies2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies=pd.concat((X_dummies1,X_dummies2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef05c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=pd.concat((X_num,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccac1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c747",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbbd01",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56df25e",
   "metadata": {},
   "source": [
    "Once all the engineering and the pre-processing is finished, we, as usual run and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824999d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_final,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0370e0",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65096d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4a6a1",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62378b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING-ADAPTING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs1 = SFS(linear_model.LinearRegression(),\n",
    "            k_features=5,\n",
    "            forward=True,\n",
    "            scoring='r2',\n",
    "            cv=3)\n",
    "sfs1 = sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4137ef",
   "metadata": {},
   "source": [
    "## Testing other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model', 'tree_Regressor']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),tree.DecisionTreeRegressor()\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26a30e",
   "metadata": {},
   "source": [
    "# Scale all of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6088da",
   "metadata": {},
   "source": [
    "For the scalling methods, this are going to be our different approachs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmin_scaler (X):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    X_scaled = MinMaxScaler().fit(X).transform(X)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "def st_scaler (X):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    X_scaled_st = StandardScaler().fit(X).transform(X)\n",
    "    return X_scaled_st\n",
    "\n",
    "def rob_scaler (X):\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    X_scaled_rob = RobustScaler().fit(X).transform(X)\n",
    "    return X_scaled_rob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fd806",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=maxmin_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584942e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler=MinMaxScaler()\n",
    "#X_scaled=scaler.fit_transform(X_num)\n",
    "#X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43703611",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bb368",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656037b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480df7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd989a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b946183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5065897",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=st_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a075cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd98ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1646c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262f64f",
   "metadata": {},
   "source": [
    "## RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=rob_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98136d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3eeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85a482",
   "metadata": {},
   "source": [
    "# MODEL 2 - Setting distance to center as dummie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1b205",
   "metadata": {},
   "source": [
    "In our second model, we tried to think how we could improve the score we obtained on our previous model. As we commented before, one of the main issues in our previous model was deling with some many subsamples due to the dummification od the zipcode variable. We therefore, tried to reduce the number of subsamples by some kind of aggrupation that had more observations for each subsample. That's why we decided to divide the zipcodes in 5 groups depending how far away were they from the most expensive area(best place to live) since it appear to be a clear pattern that distance to this point would mean less value of the house (less services, more distance to business area...). That's why we created the distance_to_center column. For the rest of the model, we just follow the same steps as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_excel('regression_data_distance_center.xls')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca29231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.loc[df3['bedrooms']==11].index, inplace=True)\n",
    "df3['bedrooms'] = df3['bedrooms'].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.loc[df3['bedrooms']==11].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3[(np.abs(stats.zscore(df3['sqft_living'])) < 3)]\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop_duplicates(subset=['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ace619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['bathrooms']=df3['bathrooms'].astype(int) \n",
    "df3['floors']=df3['floors'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3545241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['id'],axis=1)\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b615227",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat3=df3[['grade', 'Decade Build', 'condition','distance_to_center']].astype(str)\n",
    "X_cat3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies10=pd.get_dummies(X_cat3, drop_first=True)\n",
    "X_dummies10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num2=df[['sqft_living','bedrooms','bathrooms','floors', 'sqft_basement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final2=pd.concat((X_num2,X_dummies10),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ef133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df3['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73644a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_final2,y2,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe239f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2=lm.predict(X_test)\n",
    "preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7265146",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, preds2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf848e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING-ADAPTING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs1 = SFS(linear_model.LinearRegression(),\n",
    "            k_features=23,\n",
    "            forward=True,\n",
    "            scoring='r2',\n",
    "            cv=3).fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252975b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dc2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
